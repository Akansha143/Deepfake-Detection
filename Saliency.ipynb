{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 23:29:21.722 Python[99850:4098687] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Error generating saliency map with bounding boxes: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:661: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
      "\n",
      "Real frames: 2, Fake frames: 14\n",
      "Accuracy: 12.50%\n",
      "Video is classified as: Fake with 12.50% accuracy\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define constants\n",
    "BASE_DIR = \"/Users/ananyapurkait/Study Files and Folders/Semester Study/Sem VII/CS435/Project/working/code\"\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"convnext_scenario_1.h5\")\n",
    "VIDEO_FOLDER = os.path.join(BASE_DIR, \"deepfake_vids\")\n",
    "VIDEO_NAME = \"deepfake_vid2.mp4\"\n",
    "VIDEO_PATH = os.path.join(VIDEO_FOLDER, VIDEO_NAME)\n",
    "\n",
    "# Pre-trained ConvNeXt Tiny model structure\n",
    "try:\n",
    "    model = models.convnext_tiny(pretrained=False)  # Load an empty ConvNeXt model\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, weights_only=True))  # Load trained weights\n",
    "    model.classifier[2] = nn.Linear(model.classifier[2].in_features, 1)  # Modify output layer\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define preprocessing transformations\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to preprocess a frame\n",
    "def preprocess_frame(frame):\n",
    "    try:\n",
    "        frame_tensor = preprocess(frame)\n",
    "        return frame_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to generate a saliency map and draw bounding boxes\n",
    "def generate_saliency_map_with_bboxes(frame, model):\n",
    "    try:\n",
    "        input_tensor = preprocess_frame(frame)\n",
    "        input_tensor.requires_grad = True  # Enable gradient computation\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(input_tensor)\n",
    "        prediction = torch.sigmoid(output).squeeze()\n",
    "\n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        prediction.backward()\n",
    "\n",
    "        # Compute saliency map\n",
    "        saliency = input_tensor.grad.abs().squeeze().cpu().numpy()\n",
    "        saliency = np.max(saliency, axis=0)  # Aggregate across channels\n",
    "        saliency = cv2.normalize(saliency, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "        # Threshold saliency map to find regions of interest\n",
    "        _, thresh = cv2.threshold(saliency, 100, 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Generate heatmap\n",
    "        heatmap = cv2.applyColorMap(saliency, cv2.COLORMAP_JET)\n",
    "        overlay = cv2.addWeighted(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "        # Draw bounding boxes around salient regions\n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(overlay, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        return overlay, saliency\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating saliency map with bounding boxes: {e}\")\n",
    "        return frame, None\n",
    "\n",
    "# Modified deepfake detection function\n",
    "def detect_deepfake_with_bboxes(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count, fake_count, real_count = 0, 0, 0\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return None\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        if frame_count % 10 == 0:  # Process every 10th frame\n",
    "            processed_frame = preprocess_frame(frame)\n",
    "            if processed_frame is None:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                outputs = model(processed_frame)\n",
    "                prediction = torch.sigmoid(outputs).item()\n",
    "            if prediction > 0.5:\n",
    "                fake_count += 1\n",
    "            else:\n",
    "                real_count += 1\n",
    "            \n",
    "            # Display saliency map with bounding boxes\n",
    "            overlay_frame, _ = generate_saliency_map_with_bboxes(frame, model)\n",
    "            cv2.imshow(\"Deepfake Detection with Bounding Boxes\", overlay_frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_frames = fake_count + real_count\n",
    "    accuracy = (real_count / total_frames) * 100 if total_frames > 0 else 0\n",
    "    print(f\"Real frames: {real_count}, Fake frames: {fake_count}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    return \"Fake\" if fake_count > real_count else \"Real\", accuracy\n",
    "\n",
    "# Run the modified detection\n",
    "result, accuracy = detect_deepfake_with_bboxes(VIDEO_PATH)\n",
    "print(f\"Video is classified as: {result} with {accuracy:.2f}% accuracy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real frames: 13, Fake frames: 3\n",
      "Accuracy: 81.25%\n",
      "Video is classified as: Real with 81.25% accuracy\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define constants\n",
    "BASE_DIR = \"/Users/ananyapurkait/Study Files and Folders/Semester Study/Sem VII/CS435/Project/working/code\"\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"convnext_scenario_1.h5\")\n",
    "VIDEO_FOLDER = os.path.join(BASE_DIR, \"deepfake_vids\")\n",
    "VIDEO_NAME = \"deepfake_vid2.mp4\"\n",
    "VIDEO_PATH = os.path.join(VIDEO_FOLDER, VIDEO_NAME)\n",
    "\n",
    "# Pre-trained ConvNeXt Tiny model structure\n",
    "try:\n",
    "    model = models.convnext_tiny(pretrained=False)  # Load an empty ConvNeXt model\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, weights_only=True))  # Load trained weights\n",
    "    model.classifier[2] = nn.Linear(model.classifier[2].in_features, 1)  # Modify output layer\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define preprocessing transformations\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to preprocess a frame\n",
    "def preprocess_frame(frame):\n",
    "    try:\n",
    "        frame_tensor = preprocess(frame)\n",
    "        return frame_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_saliency_map_with_bboxes(frame, model):\n",
    "    try:\n",
    "        input_tensor = preprocess_frame(frame)\n",
    "        input_tensor.requires_grad = True  # Enable gradient computation\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(input_tensor)\n",
    "        prediction = torch.sigmoid(output).squeeze()\n",
    "\n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        prediction.backward()\n",
    "\n",
    "        # Compute saliency map\n",
    "        saliency = input_tensor.grad.abs().squeeze().cpu().numpy()\n",
    "        saliency = np.max(saliency, axis=0)  # Aggregate across channels\n",
    "        saliency = cv2.normalize(saliency, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "        # Threshold saliency map to find regions of interest\n",
    "        _, thresh = cv2.threshold(saliency, 100, 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Generate heatmap\n",
    "        heatmap = cv2.applyColorMap(saliency, cv2.COLORMAP_JET)\n",
    "\n",
    "        # Resize heatmap to match frame size\n",
    "        heatmap_resized = cv2.resize(heatmap, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "        # Convert the frame to RGB (if it is not already)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Overlay heatmap on the frame\n",
    "        overlay = cv2.addWeighted(frame_rgb, 0.6, heatmap_resized, 0.4, 0)\n",
    "\n",
    "        # Draw bounding boxes around salient regions\n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(overlay, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        return overlay, saliency\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating saliency map with bounding boxes: {e}\")\n",
    "        return frame, None\n",
    "\n",
    "\n",
    "# Modified deepfake detection function\n",
    "def detect_deepfake_with_bboxes(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count, fake_count, real_count = 0, 0, 0\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return None\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        if frame_count % 10 == 0:  # Process every 10th frame\n",
    "            processed_frame = preprocess_frame(frame)\n",
    "            if processed_frame is None:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                outputs = model(processed_frame)\n",
    "                prediction = torch.sigmoid(outputs).item()\n",
    "            if prediction > 0.5:\n",
    "                fake_count += 1\n",
    "            else:\n",
    "                real_count += 1\n",
    "            \n",
    "            # Display saliency map with bounding boxes\n",
    "            overlay_frame, _ = generate_saliency_map_with_bboxes(frame, model)\n",
    "            cv2.imshow(\"Deepfake Detection with Bounding Boxes\", overlay_frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_frames = fake_count + real_count\n",
    "    accuracy = (real_count / total_frames) * 100 if total_frames > 0 else 0\n",
    "    print(f\"Real frames: {real_count}, Fake frames: {fake_count}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    return \"Fake\" if fake_count > real_count else \"Real\", accuracy\n",
    "\n",
    "# Run the modified detection\n",
    "result, accuracy = detect_deepfake_with_bboxes(VIDEO_PATH)\n",
    "print(f\"Video is classified as: {result} with {accuracy:.2f}% accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

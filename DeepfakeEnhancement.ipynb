{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07714fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "452238f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "import seaborn as sb\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "from torchvision.utils import make_grid\n",
    "from torch.autograd import Variable\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\n",
    "import shutil  ##offers high-level operation on a file like a copy, create, and remote operation on the file\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d420120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"working\"):\n",
    "    os.mkdir(\"working\")\n",
    "code_dir = \"./working/code\"\n",
    "model_dir = \"./working/model\"\n",
    "output_dir = \"./working/output\"\n",
    "\n",
    "if not os.path.exists(code_dir):\n",
    "    os.mkdir(code_dir)\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b283b614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ananyapurkait/Study Files and Folders/Semester Study/Sem VII/CS435/Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70dd4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"working/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3fc4d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ananyapurkait/Study Files and Folders/Semester Study/Sem VII/CS435/Project/working/code'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c743ce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../model\")\n",
    "import convnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "265266a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models import create_model\n",
    "# def ConvNeXt_model():\n",
    "#     # Instantiate ConvNeXt Tiny with pretrained weights\n",
    "#     model = create_model('convnext_tiny', pretrained=True)\n",
    "#     return model\n",
    "\n",
    "#Instantiating the ConvNeXt model\n",
    "def ConvNeXt_model():\n",
    "    model_conv=convnext.ConvNeXt()\n",
    "    #model_conv = create_model('convnext_tiny', pretrained=True)\n",
    "    state_dict = torch.load('../../model/convnext_tiny_1k_224_ema.pth')\n",
    "    model_conv.load_state_dict(state_dict[\"model\"])\n",
    "    \n",
    "    return model_conv\n",
    "\n",
    "def ViT_model():\n",
    "    model_vit=vit_b_16(pretrained=True)\n",
    "    return model_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e9b6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30af9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50841075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "307faa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, teste_acc, train_loss, teste_loss = [], [], [], []\n",
    "train_precision, teste_precision, train_recall, teste_recall = [], [], [], []\n",
    "train_f1, teste_f1 = [], []\n",
    "df = pd.DataFrame(columns=['Modelo','Experimento','Epoch', 'Train ACC', 'Train Loss', 'Train F1', 'Test ACC', 'Test Loss', 'Test F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b91bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_data_transform(model_type, data_fraction, batch_size):\n",
    "    # Data transformations\n",
    "    if model_type== 'convnext':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    elif model_type == 'vit':\n",
    "        transform = ViT_B_16_Weights.IMAGENET1K_V1.transforms()\n",
    "            \n",
    "            \n",
    "    local_arquivos='../../newdata/real_vs_fake/real-vs-fake'\n",
    "    full_train_dataset = ImageFolder(local_arquivos + \"/train\", transform=transform)\n",
    "    full_test_dataset = ImageFolder(local_arquivos + \"/test\", transform=transform)\n",
    "    # Determine the number of objects to be selected\n",
    "    num_train_data = int(len(full_train_dataset) * data_fraction)\n",
    "    num_test_data = int(len(full_test_dataset) * data_fraction)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Randomly select objects for the datasets\n",
    "    train_indices = random.sample(range(len(full_train_dataset)), num_train_data)\n",
    "    test_indices = random.sample(range(len(full_test_dataset)), num_test_data)\n",
    "    # Create datasets with randomly selected objects\n",
    "    train_dataset = torch.utils.data.Subset(full_train_dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(full_test_dataset, test_indices)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=int(batch_size/2), shuffle=False)\n",
    "        \n",
    "        \n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "222fa8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_data_transform(model_type, data_fraction, batch_size):\n",
    "    # Data transformations\n",
    "    if model_type== 'convnext':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    elif model_type == 'vit':\n",
    "        transform = ViT_B_16_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "    local_arquivos='../../newdata/real_vs_fake/real-vs-fake'\n",
    "    full_test_dataset = ImageFolder(local_arquivos + \"/test\", transform=transform)\n",
    "            \n",
    "    # Determine the number of objects to be selected\n",
    "    num_test_data = int(len(full_test_dataset) * data_fraction)\n",
    "    # Randomly select objects for the datasets\n",
    "    test_indices = random.sample(range(len(full_test_dataset)), num_test_data)\n",
    "\n",
    "    # Create datasets with randomly selected objects\n",
    "    test_dataset = torch.utils.data.Subset(full_test_dataset, test_indices)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=int(batch_size/2), shuffle=False)\n",
    "\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94f87889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AutoAugment_transform(model_type, train_indices, batch_size):\n",
    "    if model_type== 'convnext':\n",
    "        augmentation_transforms = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.AutoAugment(), ## for automatically enhancing the diversity and quality of the training data\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    elif model_type == 'vit':\n",
    "        augmentation_transforms = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=Image.BILINEAR),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.AutoAugment(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "        ])\n",
    "    local_arquivos='../../newdata/real_vs_fake/real-vs-fake'\n",
    "    augmented_train_dataset = ImageFolder(local_arquivos + \"/train\", transform=augmentation_transforms)\n",
    "    augmented_train_dataset = torch.utils.data.Subset(augmented_train_dataset, train_indices)\n",
    "    augmented_train_dataloader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return augmented_train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2af76f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandAugment_transform(model_type, train_indices, batch_size):\n",
    "    if model_type== 'convnext':\n",
    "        augmentation_transforms = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.RandAugment(),#RandAugment simplifies the process by using a reduced search space. Instead of searching for the best policy from scratch\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    elif model_type == 'vit':\n",
    "        augmentation_transforms = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=Image.BILINEAR),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.RandAugment(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "        ])\n",
    "        \n",
    "    local_arquivos='../../newdata/real_vs_fake/real-vs-fake'\n",
    "    augmented_train_dataset = ImageFolder(local_arquivos + \"/train\", transform=augmentation_transforms)\n",
    "    augmented_train_dataset = torch.utils.data.Subset(augmented_train_dataset, train_indices)\n",
    "    augmented_train_dataloader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return augmented_train_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "905784a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Auto_RandAugment_transform(model_type, train_indices, batch_size):\n",
    "    if model_type== 'convnext':\n",
    "        augmentation_transforms = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.AutoAugment(),\n",
    "            transforms.RandAugment(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    elif model_type == 'vit':\n",
    "        augmentation_transforms = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=Image.BILINEAR),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.AutoAugment(),\n",
    "            transforms.RandAugment(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "        ])\n",
    "        \n",
    "    local_arquivos='../../newdata/real_vs_fake/real-vs-fake'\n",
    "    augmented_train_dataset = ImageFolder(local_arquivos + \"/train\", transform=augmentation_transforms)\n",
    "    augmented_train_dataset = torch.utils.data.Subset(augmented_train_dataset, train_indices)\n",
    "    augmented_train_dataloader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return augmented_train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8560e77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ananyapurkait/Study Files and Folders/Semester Study/Sem VII/CS435/Project/working/code'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2747249a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Generic training function\n",
    "def train(model, dataloader, criterion, optimizer, scheduler, device, ft, epoch, exp, model_type):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true,y_pred=[], []\n",
    "    \n",
    "    loop = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for batch_idx, (images, labels) in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() #Clears the gradients of all optimized tensors before performing backpropagation.\n",
    "        if ft==False:    #Conditionally executes either fine-tuning or regular training based on the ft flag\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad=False   #Freeze model parameters\n",
    "                \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.step()\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad=True\n",
    "            #After the optimization step, this loop iterates over all parameters of the model again and sets requires_grad attribute to True. This unfreezes the model's weights and biases, allowing them to be updated during subsequent iterations of training.\n",
    "        else:    \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = outputs.argmax(dim = 1)\n",
    "    \n",
    "        y_true.extend(labels.cpu().tolist())\n",
    "        y_pred.extend(predicted.cpu().tolist())\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        loop.set_description(f\"[Epoch {(epoch+1)}]\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "        \n",
    "    train_loss = running_loss / len(dataloader.dataset)  \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro',zero_division=0)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.6f} | Train Accuracy: {(accuracy * 100):.2f}% | Train F1-Score: {f1:.6f}\")\n",
    "    \n",
    "    model_name= f'model_{model_type}_params_exp_{exp}.pth'\n",
    "    torch.save(model.state_dict(), os.path.join('../../model', model_name))\n",
    "   \n",
    "    return train_loss, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db94c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic testing function\n",
    "def test(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_pred, y_true= [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted = outputs.argmax(dim = 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().tolist())\n",
    "            y_pred.extend(predicted.cpu().tolist())\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss = running_loss / len(list(dataloader.dataset))\n",
    "    test_accuracy = accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, test_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro',zero_division=0)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.6f} | Test Accuracy: {(test_accuracy * 100):.2f}% | Test F1-Score: {test_f1:.6f}\")\n",
    "    return test_loss, test_accuracy, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34dee0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generic function for training and testing\n",
    "def train_model(model_type, exp,model, train_dataloader, test_dataloader, criterion, optimizer, scheduler, device, num_epochs, ft, num):\n",
    "    model=model.to(device)\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        train_loss, train_accuracy, train_f1 = train(model, train_dataloader, criterion, optimizer, scheduler, device, ft, epoch, exp, model_type)\n",
    "        test_loss, test_accuracy,test_f1 = test(model, test_dataloader, criterion, device)\n",
    "        val=str(num)+str(epoch+1)\n",
    "        df.loc[val]=[model_type, exp, epoch+1, train_accuracy, train_loss, train_f1, test_accuracy, test_loss, test_f1]\n",
    "        df.to_csv('metricas.csv', index = False)\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "    return train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc973248-c7f5-44ae-a128-a4d9565a6e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scenario(model_type, scenario, data_fraction, num_epochs=10, batch_size=32, learning_rate=0.001):\n",
    "    num_classes = 2  \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    num_train_data = int((100000) * data_fraction)\n",
    "    num_test_data = int((20000) * data_fraction)\n",
    "    \n",
    "    train_indices = random.sample(range(65000), num_train_data)\n",
    "    test_indices = random.sample(range(20000), num_test_data)\n",
    "    \n",
    "    print(f'Number of training objects: {num_train_data}')\n",
    "    print(f'Number of test objects: {num_test_data}')\n",
    "    print(\"============================================================================\")\n",
    "    \n",
    "    if scenario == 1:\n",
    "        \n",
    "        train_dataloader, test_dataloader= full_data_transform(model_type, data_fraction, batch_size)\n",
    "        \n",
    "        \n",
    "        if model_type == 'convnext':\n",
    "            model = ConvNeXt_model()\n",
    "            model.head = nn.Linear(model.head.in_features, num_classes)  # Substituir num_classes pelo número correto de classes\n",
    "            num=1\n",
    "\n",
    "        elif model_type == 'vit':\n",
    "            model = ViT_model()\n",
    "            model.heads=nn.Linear(768,2)\n",
    "            num=9\n",
    "        else:\n",
    "            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
    "        \n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "        if model_type == 'convnext':\n",
    "            print(\"Experimento 1 - ConvNeXt (Sem Fine-Tuning)\")\n",
    "        elif model_type == 'vit':\n",
    "            print(\"Experimento 1 - ViT (Sem Fine-Tuning)\")\n",
    "        \n",
    "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,1, model, train_dataloader, test_dataloader,\n",
    "                                                                                    criterion, optimizer, scheduler, device, 1, False, num)\n",
    "    elif scenario == 2:\n",
    "        \n",
    "        train_dataloader, test_dataloader= full_data_transform(model_type, data_fraction, batch_size)\n",
    "        \n",
    "       \n",
    "        if model_type == 'convnext':\n",
    "            model = ConvNeXt_model()\n",
    "            model.head = nn.Linear(model.head.in_features, num_classes)  # Substituir num_classes pelo número correto de classes\n",
    "            num=2\n",
    "\n",
    "        elif model_type == 'vit':\n",
    "            model = ViT_model()\n",
    "            model.heads=nn.Linear(768,2)\n",
    "            num=10\n",
    "        else:\n",
    "            raise ValueError(\"O parâmetro 'model_type' deve ser 'convnext' ou 'vit'.\")\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "        if model_type == 'convnext':\n",
    "             print(\"Experiment 2 - ConvNeXt (With Fine-Tuning)\")\n",
    "        elif model_type == 'vit':\n",
    "            print(\"Experiment 2 - ViT (With Fine-Tuning)\")\n",
    "        \n",
    "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,2, model, train_dataloader, test_dataloader,\n",
    "                                                                                    criterion, optimizer, scheduler, device, num_epochs, True, num)\n",
    "    elif scenario == 3:\n",
    "        \n",
    "        augmented_train_dataloader = AutoAugment_transform(model_type, train_indices, batch_size)\n",
    "        test_dataloader=ft_data_transform(model_type, data_fraction, batch_size)\n",
    "        \n",
    "\n",
    "        if model_type == 'convnext':\n",
    "            model = ConvNeXt_model()\n",
    "            model.head = nn.Linear(model.head.in_features, num_classes)  # Substituir num_classes pelo número correto de classes\n",
    "            num=3\n",
    "\n",
    "        elif model_type == 'vit':\n",
    "            model = ViT_model()\n",
    "            model.heads=nn.Linear(768,2)\n",
    "            num=11\n",
    "        else:\n",
    "            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "        if model_type == 'convnext':\n",
    "            print(\"Experiment 3 - ConvNeXt (No Fine-Tuning with AutoAugment\")\n",
    "        elif model_type == 'vit':\n",
    "            print(\"Experiment 3 - ViT (No Fine-Tuning with AutoAugment)\")\n",
    "        \n",
    "        \n",
    "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,3, model, augmented_train_dataloader, test_dataloader,\n",
    "                                                                                   criterion, optimizer, scheduler, device, 1, False, num)\n",
    "      \n",
    "        if model_type == 'convnext':\n",
    "            model = ConvNeXt_model()\n",
    "            model.head = nn.Linear(model.head.in_features, num_classes)  # Replace num_classes with the correct number of classes\n",
    "            num=4\n",
    "            \n",
    "        elif model_type == 'vit':\n",
    "            model = ViT_model()\n",
    "            model.heads=nn.Linear(768,2)\n",
    "            num=12\n",
    "        else:\n",
    "            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "        if model_type == 'convnext':\n",
    "            print(\"Experiment 4 - ConvNeXt (With Fine-Tuning and with AutoAugment)\")\n",
    "        elif model_type == 'vit':\n",
    "            print(\"Experiment 4 - ViT (With Fine-Tuning and with AutoAugment)\")\n",
    "        \n",
    "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,4, model, augmented_train_dataloader, test_dataloader,\n",
    "                                                                                   criterion, optimizer, scheduler, device, num_epochs, True, num)\n",
    "    \n",
    "    elif scenario == 4:\n",
    "        augmented_train_dataloader = RandAugment_transform(model_type, train_indices, batch_size)\n",
    "        test_dataloader=ft_data_transform(model_type, data_fraction, batch_size)\n",
    "        \n",
    "        if model_type == 'convnext':\n",
    "            model = ConvNeXt_model()\n",
    "            model.head = nn.Linear(model.head.in_features, num_classes)  # Substituir num_classes pelo número correto de classes\n",
    "            num=5\n",
    "            \n",
    "        elif model_type == 'vit':\n",
    "            model = ViT_model()\n",
    "            model.heads=nn.Linear(768,2)\n",
    "            num=13\n",
    "        else:\n",
    "            raise ValueError(\"O parâmetro 'model_type' deve ser 'convnext' ou 'vit'.\")\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "  \n",
    "        if model_type == 'convnext':\n",
    "            print(\"Experiment 5 - ConvNeXt (No Fine-Tuning and with RandAugment)\")\n",
    "        elif model_type == 'vit':\n",
    "            print(\"Experiment 5 - ViT (No Fine-Tuning and with RandAugment)\")    \n",
    "        \n",
    "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,5, model, augmented_train_dataloader, test_dataloader,\n",
    "                                                                                   criterion, optimizer, scheduler, device, 1, False, num)\n",
    "        \n",
    "        if model_type == 'convnext':\n",
    "            model = ConvNeXt_model()\n",
    "            model.head = nn.Linear(model.head.in_features, num_classes)  #Replace num_classes with the correct number of classes\n",
    "            num=6\n",
    "            \n",
    "        elif model_type == 'vit':\n",
    "            model = ViT_model()\n",
    "            model.heads=nn.Linear(768,2)\n",
    "            num=14\n",
    "        else:\n",
    "            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "        if model_type == 'convnext':\n",
    "            print(\"Experiment 6 - ConvNeXt (With Fine-Tuning and with RandAugment)\")\n",
    "        elif model_type == 'vit':\n",
    "            print(\"Experiment 6 - ViT (With Fine-Tuning and with RandAugment)\")\n",
    "        \n",
    "        \n",
    "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,6, model, augmented_train_dataloader, test_dataloader,\n",
    "                                                                                   criterion, optimizer, scheduler, device, num_epochs, True, num) \n",
    "\n",
    "    elif scenario == 5:\n",
    "        augmented_train_dataloader = Auto_RandAugment_transform(model_type, train_indices, batch_size)\n",
    "        test_dataloader=ft_data_transform(model_type, data_fraction, batch_size)\n",
    "        \n",
    "        if model_type == 'convnext':\n",
    "            model = ConvNeXt_model()\n",
    "            model.head = nn.Linear(model.head.in_features, num_classes)  # Substituir num_classes pelo número correto de classes\n",
    "            num=7\n",
    "            \n",
    "        elif model_type == 'vit':\n",
    "            model = ViT_model()\n",
    "            model.heads=nn.Linear(768,2)\n",
    "            num=15\n",
    "        else:\n",
    "            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "        if model_type == 'convnext':\n",
    "            print(\"Experiment 7 - ConvNeXt (No Fine-Tuning and with RandAugment and AutoAugment)\")\n",
    "        elif model_type == 'vit':\n",
    "            print(\"Experiment 7 - ViT (No Fine-Tuning and with RandAugment and AutoAugment)\")\n",
    "            \n",
    "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,7, model, augmented_train_dataloader, test_dataloader,\n",
    "                                                                                   criterion, optimizer, scheduler, device, 1, False, num)\n",
    "      \n",
    "        # Select the model\n",
    "        if model_type == 'convnext':\n",
    "            model = ConvNeXt_model()\n",
    "            model.head = nn.Linear(model.head.in_features, num_classes) \n",
    "            num=8\n",
    "            \n",
    "        elif model_type == 'vit':\n",
    "            model = ViT_model()\n",
    "            model.heads=nn.Linear(768,2)\n",
    "            num=16\n",
    "        else:\n",
    "            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "        if model_type == 'convnext':\n",
    "            print(\"Experiment 8 - ConvNeXt (With Fine-Tuning and with RandAugment and AutoAugment)\")\n",
    "        elif model_type == 'vit':\n",
    "            print(\"Experiment 8 - ViT (With Fine-Tuning and with RandAugment and AutoAugment)\")\n",
    "\n",
    "        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,8, model, augmented_train_dataloader, test_dataloader,\n",
    "                                                                                   criterion, optimizer, scheduler, device, num_epochs, True, num)\n",
    "    else:\n",
    "        raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19dca9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_experiment_graphs(df, experiment_number):\n",
    "    experiment_df = df[df['Experimento'] == experiment_number]\n",
    "\n",
    "    #  Plot epoch x train F1\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(experiment_df['Epoch'], experiment_df['Train F1'], marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train F1')\n",
    "    plt.title(f'Experiment {experiment_number} - Train F1')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot epoch x train accuracy\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(experiment_df['Epoch'], experiment_df['Train ACC'], marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Accuracy')\n",
    "    plt.title(f'Experiment {experiment_number} - Train Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot epoch x train loss\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(experiment_df['Epoch'], experiment_df['Train Loss'], marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.title(f'Experimento {experiment_number} - Trai loss')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot epoch x test F1\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(experiment_df['Epoch'], experiment_df['Test F1'], marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Test F1')\n",
    "    plt.title(f'Experiment {experiment_number} - Test F1')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot epoch x test accuracy\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(experiment_df['Epoch'], experiment_df['Test ACC'], marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.title(f'Experiment {experiment_number} - Test Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot epoch x test loss\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(experiment_df['Epoch'], experiment_df['Test Loss'], marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Test loss')\n",
    "    plt.title(f'Experimento {experiment_number} - Test loss')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_general_graphs(df):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "    # Plot epoch x train F1\n",
    "    ax1 = axes[0, 0]\n",
    "    for experiment_number in df['Experimento'].unique():\n",
    "        experiment_df = df[df['Experimento'] == experiment_number]\n",
    "        ax1.plot(experiment_df['Epoch'], experiment_df['Train F1'], marker='o', label=f'Experimento {experiment_number}')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Train F1')\n",
    "    ax1.set_title('Overall - Train F1')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot epoch x train loss\n",
    "    ax2 = axes[0, 1]\n",
    "    for experiment_number in df['Experiment'].unique():\n",
    "        experiment_df = df[df['Experiment'] == experiment_number]\n",
    "        ax2.plot(experiment_df['Epoch'], experiment_df['Train Loss'], marker='o', label=f'Experiment {experiment_number}')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Train Loss')\n",
    "    ax2.set_title('Overall - Train Loss')\n",
    "    ax2.legend()\n",
    "\n",
    "    # Plot epoch x test F1\n",
    "    ax3 = axes[1, 0]\n",
    "    for experiment_number in df['Experimento'].unique():\n",
    "        experiment_df = df[df['Experimento'] == experiment_number]\n",
    "        ax3.plot(experiment_df['Epoch'], experiment_df['Test F1'], marker='o', label=f'Experimento {experiment_number}')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Test F1')\n",
    "    ax3.set_title('Overall - Test F1')\n",
    "    ax3.legend()\n",
    "\n",
    "    # Plot epoch x test loss\n",
    "    ax4 = axes[1, 1]\n",
    "    for experiment_number in df['Experiment'].unique():\n",
    "        experiment_df = df[df['Experiment'] == experiment_number]\n",
    "        ax4.plot(experiment_df['Epoch'], experiment_df['Test Loss'], marker='o', label=f'Experimento {experiment_number}')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Test Loss')\n",
    "    ax4.set_title('Overall - Test Loss')\n",
    "    ax4.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbc40c-135b-4d9f-b3b7-3168761b4c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "929128f1-de46-48ad-b949-0eba7a441dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# After each run of the scenario, save the model to a .h5 file\n",
    "def save_model_as_h5(model, filename):\n",
    "    # Save model state_dict in an h5 file using h5py\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        # Convert state_dict to numpy arrays\n",
    "        for name, param in model.state_dict().items():\n",
    "            f.create_dataset(name, data=param.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee9cca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training objects: 10000\n",
      "Number of test objects: 2000\n",
      "============================================================================\n",
      "Experimento 1 - ConvNeXt (Sem Fine-Tuning)\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1]: 100%|██████████████████| 625/625 [27:50<00:00,  2.67s/it, loss=0.667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.043636 | Train Accuracy: 51.34% | Train F1-Score: 0.449627\n",
      "Test Loss: 0.087110 | Test Accuracy: 51.70% | Test F1-Score: 0.446903\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_scenario('convnext',1,0.1, num_epochs=10, batch_size=16, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5acfb2f7-368e-445e-97e8-bef06323a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Example: Initializing a model\n",
    "mlmodel = models.convnext_tiny(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d42a8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlmodel.state_dict(), \"convnext_scenario_1.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7acc759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Modelo  Experimento  Epoch  Train ACC  Train Loss  Train F1  Test ACC  \\\n",
      "11  convnext            1      1     0.5134    0.043636  0.449627     0.517   \n",
      "\n",
      "    Test Loss   Test F1  \n",
      "11    0.08711  0.446903  \n"
     ]
    }
   ],
   "source": [
    "mean_df = df.groupby(['Modelo', 'Experimento']).mean()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984495b6-7d07-447c-81a5-3a3d1a606e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_as_h5(mlmodel, \"convnext_scenario_1_model_epoch_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46024b12-0bb6-4166-af68-83c224c548a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training objects: 10000\n",
      "Number of test objects: 2000\n",
      "============================================================================\n",
      "Experiment 2 - ConvNeXt (With Fine-Tuning)\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1]: 100%|██████████████| 625/625 [2:24:25<00:00, 13.86s/it, loss=0.00763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.007432 | Train Accuracy: 94.92% | Train F1-Score: 0.949198\n",
      "Test Loss: 0.010563 | Test Accuracy: 97.55% | Test F1-Score: 0.975485\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2]: 100%|█████████████| 625/625 [2:29:58<00:00, 14.40s/it, loss=0.000356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.001032 | Train Accuracy: 99.37% | Train F1-Score: 0.993700\n",
      "Test Loss: 0.000715 | Test Accuracy: 99.80% | Test F1-Score: 0.998000\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3]: 100%|██████████████| 625/625 [2:04:16<00:00, 11.93s/it, loss=0.00016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000075 | Train Accuracy: 99.97% | Train F1-Score: 0.999700\n",
      "Test Loss: 0.000617 | Test Accuracy: 99.80% | Test F1-Score: 0.998000\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4]: 100%|█████████████| 625/625 [1:41:50<00:00,  9.78s/it, loss=0.000351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000022 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
      "Test Loss: 0.000369 | Test Accuracy: 99.85% | Test F1-Score: 0.998500\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 5]: 100%|█████████████| 625/625 [4:48:43<00:00, 27.72s/it, loss=0.000267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000016 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
      "Test Loss: 0.000364 | Test Accuracy: 99.85% | Test F1-Score: 0.998500\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 6]: 100%|█████████████| 625/625 [2:36:26<00:00, 15.02s/it, loss=0.000273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000015 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
      "Test Loss: 0.000358 | Test Accuracy: 99.90% | Test F1-Score: 0.999000\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 7]: 100%|█████████████| 625/625 [6:20:11<00:00, 36.50s/it, loss=0.000293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000015 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
      "Test Loss: 0.000357 | Test Accuracy: 99.90% | Test F1-Score: 0.999000\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 8]: 100%|████████████| 625/625 [13:24:51<00:00, 77.27s/it, loss=0.000526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000015 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
      "Test Loss: 0.000356 | Test Accuracy: 99.90% | Test F1-Score: 0.999000\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 9]: 100%|████████████| 625/625 [13:47:20<00:00, 79.42s/it, loss=0.000122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000015 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
      "Test Loss: 0.000356 | Test Accuracy: 99.90% | Test F1-Score: 0.999000\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 10]: 100%|███████████| 625/625 [11:30:13<00:00, 66.26s/it, loss=0.000147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.000015 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\n",
      "Test Loss: 0.000356 | Test Accuracy: 99.90% | Test F1-Score: 0.999000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_scenario('convnext',2,0.1, num_epochs=10, batch_size=16, learning_rate=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2076451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Epoch  Train ACC  Train Loss  Train F1  Test ACC  \\\n",
      "Modelo   Experimento                                                     \n",
      "convnext 1              1.0    0.51340    0.043636  0.449627   0.51700   \n",
      "         2              5.5    0.99426    0.000865  0.994260   0.99635   \n",
      "         3              1.0    0.26760    0.048634  0.252744   0.49400   \n",
      "         4              1.5    0.94705    0.007732  0.919874   0.98925   \n",
      "\n",
      "                      Test Loss   Test F1  \n",
      "Modelo   Experimento                       \n",
      "convnext 1             0.087110  0.446903  \n",
      "         2             0.001441  0.996349  \n",
      "         3             0.088194  0.401016  \n",
      "         4             0.003297  0.989248  \n"
     ]
    }
   ],
   "source": [
    "mean_df = df.groupby(['Modelo', 'Experimento']).mean()\n",
    "print(mean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11069ce4-b26f-458f-bba9-21fb998a0fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_as_h5(mlmodel, \"convnext_scenario_2_model_epoch_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c08357-6a55-4e9b-8f9e-ac15881c9930",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scenario('convnext',3,0.1, num_epochs=10, batch_size=16, learning_rate=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287a5c28-dbf8-42c9-b99b-3e76ee66d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = df.groupby(['Modelo', 'Experimento']).mean()\n",
    "print(mean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d24798-d4a3-4739-b7ae-bc5921da9a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_as_h5(mlmodel, \"convnext_scenario_3_model_epoch_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf2b44-8e7d-4ff3-bf3c-e2867390da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scenario('convnext',4,0.1, num_epochs=10, batch_size=16, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285aaeb-aa25-4ace-904e-9b9d5216814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = df.groupby(['Modelo', 'Experimento']).mean()\n",
    "print(mean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef307163-ff8e-4262-b5df-2c313a51e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_as_h5(mlmodel, \"convnext_scenario_4_model_epoch_4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a75962-ec9b-4233-bd6a-14318598ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scenario('convnext',5,0.1, num_epochs=10, batch_size=16, learning_rate=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe7f81-d514-4e8d-b971-644cc106ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = df.groupby(['Modelo', 'Experimento']).mean()\n",
    "print(mean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b16f18-4a75-4dec-9fb5-44792e263be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_as_h5(mlmodel, \"convnext_scenario_5_model_epoch_5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd906f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e695d-4dc1-4349-9dff-a1d8fe10e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(mlmodel.state_dict(), \"/Users/ananyapurkait/Study Files and Folders/Semester Study/Sem VII/CS435/Project/working/code/convnext_scenario_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38eec59-d582-4804-abed-f21bf906c7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
